{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40eba46",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code the DDPG algorithm.\n",
    "\n",
    "To understand this code, you need to know more about \n",
    "[the BBRL interaction model](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing).\n",
    "Then you should run [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "to see how agents interact.\n",
    "\n",
    "You also need to understand [details about the\n",
    "AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing).\n",
    "\n",
    "The DDPG algorithm is explained in [this video](https://www.youtube.com/watch?v=0D6a0a1HTtc)\n",
    "and you can also read [the # corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d2bdc",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "### Installation\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "We use OmegaConf to that makes it possible that by just defining the `def\n",
    "run_dqn(cfg):` function and then executing a long `params = {...}` variable at\n",
    "the bottom of this colab, the code is run with the parameters without calling\n",
    "an explicit main.\n",
    "\n",
    "More precisely, the code is run by calling\n",
    "\n",
    "`config=OmegaConf.create(params)`\n",
    "\n",
    "`run_dqn(config)`\n",
    "\n",
    "at the very bottom of the colab, after starting tensorboard.\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium\n",
    "environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd60669f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e584795c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508a0284",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ... \n",
    "# \n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8265509f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 1,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "                answer = input(f\"Do you want to launch tensorboard in this notebook [y/n] \").lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "        print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e54fb",
   "metadata": {},
   "source": [
    "The [DDPG](https://arxiv.org/pdf/1509.02971.pdf) algorithm is an actor critic\n",
    "algorithm. We use a simple neural network builder function. This neural\n",
    "networks plays the role of the actor and the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3268bd",
   "metadata": {},
   "source": [
    "## Definition of agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ec662",
   "metadata": {},
   "source": [
    "The function below builds a multi-layer perceptron where the size of each layer is given in the `size` list.\n",
    "We also specify the activation function of neurons at each layer and optionally a different activation function for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501bd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c207da",
   "metadata": {},
   "source": [
    "The critic is a neural network taking the state $s$ and action $a$ as input,\n",
    "and its output layer has a unique neuron whose value is the value of being in\n",
    "that state and performing that action $Q(s,a)$.\n",
    "\n",
    "As usual, the ```forward(...)``` function is used to write Q-values in the\n",
    "workspace from time indexes, whereas the ```predict_value(...)``` function` is\n",
    "used in other contexts, such as plotting a view of the Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0678bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, name=\"critic\"):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "        self.with_name(name)\n",
    "\n",
    "    def with_name(self, name):\n",
    "        self.name = f\"{name}/q_value\"\n",
    "        return self\n",
    "\n",
    "    def forward(self, t, detach_actions=False):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        if detach_actions:\n",
    "            action = action.detach()\n",
    "        osb_act = torch.cat((obs, action), dim=1)\n",
    "        q_value = self.model(osb_act)\n",
    "        self.set((self.name, t), q_value)\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        osb_act = torch.cat((obs, action), dim=0)\n",
    "        q_value = self.model(osb_act)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd341cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The actor is also a neural network, it takes a state $s$ as input and outputs\n",
    "an action $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c0d1c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "    def predict_action(self, obs, stochastic):\n",
    "        assert (\n",
    "            not stochastic\n",
    "        ), \"ContinuousDeterministicActor cannot provide stochastic predictions\"\n",
    "        return self.model(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0655387",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Exploration method\n",
    "\n",
    "In the continuous action domain, basic exploration differs from the methods\n",
    "used in the discrete action domain. Here we generally add some Gaussian noise\n",
    "to the output of the actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb462404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c6e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888993f",
   "metadata": {},
   "source": [
    "In [the original DDPG paper](https://arxiv.org/pdf/1509.02971.pdf), the\n",
    "authors rather used the more sophisticated Ornstein-Uhlenbeck noise where\n",
    "noise is correlated between one step and the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34519ed6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3e6a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Training and evaluation environments\n",
    "\n",
    "We build two environments: one for training and another one for evaluation.\n",
    "\n",
    "For training, it is more efficient to use an autoreset agent, as we do not\n",
    "want to waste time if the task is done in an environment sooner than in the\n",
    "others.\n",
    "\n",
    "By contrast, for evaluation, we just need to perform a fixed number of\n",
    "episodes (for statistics), thus it is more convenient to use a\n",
    "noautoreset agent with a set of environments and just run one episode in\n",
    "each environment. Thus we can use the `env/done` stop variable and take the\n",
    "average over the cumulated reward of all environments.\n",
    "\n",
    "See [this\n",
    "notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "for explanations about agents and environment agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6619347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "def get_env_agents(cfg, *, autoreset=True, include_last_state=True) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, autoreset=autoreset),\n",
    "        cfg.algorithm.n_envs, \n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name), \n",
    "        cfg.algorithm.nb_evals,\n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dea82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Create the DDPG agent\n",
    "\n",
    "In this function we create the critic and the actor, but also an exploration\n",
    "agent to add noise and a target critic. The version below does not use a\n",
    "target actor as it proved hard to tune, but such a target actor is used in the\n",
    "original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f463ce6c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create the DDPG Agent\n",
    "def create_ddpg_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "    critic = ContinuousQAgent(\n",
    "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "    )\n",
    "    target_critic = copy.deepcopy(critic).with_name(\"target-critic\")\n",
    "    actor = ContinuousDeterministicActor(\n",
    "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "    )\n",
    "    # target_actor = copy.deepcopy(actor) # not used in practice, though described in the paper\n",
    "    noise_agent = AddGaussianNoise(cfg.algorithm.action_noise) # alternative : AddOUNoise\n",
    "    tr_agent = Agents(train_env_agent, actor, noise_agent)  \n",
    "    ev_agent = Agents(eval_env_agent, actor)\n",
    "\n",
    "    # Get an agent that is executed on a complete workspace\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "    return train_agent, eval_agent, actor, critic, target_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a464455",
   "metadata": {},
   "source": [
    "### The Logger class\n",
    "\n",
    "The logger is in charge of collecting statistics during the training\n",
    "process.\n",
    "\n",
    "Having logging provided under the hood is one of the features allowing you\n",
    "to save time when using RL libraries like BBRL.\n",
    "\n",
    "In these notebooks, the logger is defined as `bbrl.utils.logger.TFLogger` so as\n",
    "to use a tensorboard visualisation (see the parameters part `params = { \"logger\":{ ...` below).\n",
    "\n",
    "Note that the BBRL Logger is also saving the log in a readable format such\n",
    "that you can use `Logger.read_directories(...)` to read multiple logs, create\n",
    "a dataframe, and analyze many experiments afterward in a notebook for\n",
    "instance. The code for the different kinds of loggers is available in the\n",
    "[bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/logger.py)\n",
    "file.\n",
    "\n",
    "`instantiate_class` is an inner BBRL mechanism. The\n",
    "`instantiate_class`function is available in the\n",
    "[`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/__init__.py)\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae47ba5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bbrl import instantiate_class\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string, loss, steps):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    # A specific function for RL algorithms having a critic, an actor and an entropy losses\n",
    "    def log_losses(self, critic_loss, entropy_loss, actor_loss, steps):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168ea73",
   "metadata": {},
   "source": [
    "### Setup the optimizers\n",
    "\n",
    "We use two separate optimizers to tune the parameters of the actor and the critic separately. That makes it possible to use a different learning rate for the actor and the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0807f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizers\n",
    "def setup_optimizers(cfg, actor, critic):\n",
    "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n",
    "    parameters = actor.parameters()\n",
    "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n",
    "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n",
    "    parameters = critic.parameters()\n",
    "    critic_optimizer = get_class(cfg.critic_optimizer)(\n",
    "        parameters, **critic_optimizer_args\n",
    "    )\n",
    "    return actor_optimizer, critic_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041331e",
   "metadata": {},
   "source": [
    "### Compute critic loss\n",
    "\n",
    "Detailed explanations of the function to compute the critic loss when using a NoAutoResetGymAgent are given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing).\n",
    "\n",
    "The case where we use the AutoResetGymAgent is very similar, but we need to specify that we use the first part of the Q-values (`q_values[0]`) for representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing $Q(s_{t+1},a)$, as these values are stored into a transition model. Then the values used to compute the target in the critic update are stored into ```target_q_values```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65bd5f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):\n",
    "    # Compute temporal difference\n",
    "    q_next = target_q_values  # a .detach() is not necessary because target_q_values are computed within a with torch.no_grad\n",
    "    target = (\n",
    "        reward[1:].squeeze()\n",
    "        + cfg.algorithm.discount_factor * q_next.squeeze(-1) * must_bootstrap.int()\n",
    "    )\n",
    "    # Compute critic loss\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, q_values.squeeze(-1))\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c5d6cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Soft parameter updates\n",
    "\n",
    "To update the target critic, one uses the following equation:\n",
    "$\\theta' \\leftarrow \\tau \\theta + (1- \\tau) \\theta'$\n",
    "where $\\theta$ is the vector of parameters of the critic, and $\\theta'$ is the vector of parameters of the target critic.\n",
    "The `soft_update_params(...)` function is in charge of performing this soft update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daba51d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def soft_update_params(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e5db9",
   "metadata": {},
   "source": [
    "### Compute actor loss\n",
    "The actor loss is straightforward. We want the actor to maximize Q-values, thus we minimize the mean of negated Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d04271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    return -q_values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b18f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop\n",
    "\n",
    "### Agent execution\n",
    "\n",
    "This is the tricky part with BBRL, the one we need to understand in detail.\n",
    "The difficulty lies in the copy of the last step and the way to deal with the\n",
    "n_steps return.\n",
    "\n",
    "The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps -\n",
    "1, stochastic=True)` makes the agent run a number of steps in the workspace.\n",
    "In practice, it calls the\n",
    "[`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/agents/agent.py#L59)\n",
    "function which makes a forward pass of the agent network using the workspace\n",
    "data and updates the workspace accordingly.\n",
    "\n",
    "Now, if we start at the first epoch (`epoch=0`), we start from the first step\n",
    "(`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must\n",
    "not forget to cover the transition at the border between the previous epoch\n",
    "and the current epoch. To avoid this risk, we copy the information from the\n",
    "last time step of the previous epoch into the first time step of the next\n",
    "epoch. This is explained in more details in [a previous\n",
    "notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\n",
    "\n",
    "A [previous\n",
    "notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)\n",
    "explains a lot of these details. In particular, read it to understand the\n",
    "`execute_agents(...)` function, the `transition_workspace =\n",
    "train_workspace.get_transitions()` line and the computation of\n",
    "`must_bootstrap`.\n",
    "\n",
    "Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`\n",
    "lines. \n",
    "\n",
    "`optimizer.zero_grad()` is necessary to cancel all the gradients computed at\n",
    "the previous iterations\n",
    "\n",
    "Note the way we count the steps, to properly ignore the steps corresponding to\n",
    "a transition from an episode to the next. Note also that every\n",
    "```cfg.algorithm.eval_interval```, we evaluate the current agent and save it\n",
    "and plot it if its performance is the new best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aef3a122",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from bbrl.visu.plot_critics import plot_critic\n",
    "\n",
    "def run_ddpg(cfg):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "    \n",
    "    # 3) Create the DDPG Agent\n",
    "    (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic,\n",
    "        target_critic,\n",
    "    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)\n",
    "    ag_actor = TemporalAgent(actor)\n",
    "    # ag_target_actor = TemporalAgent(target_actor)\n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "    train_workspace = Workspace()\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(cfg.algorithm.max_epochs))\n",
    "    p=1\n",
    "    for epoch in pbar:\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps)\n",
    "        else:\n",
    "            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "\n",
    "        terminated, reward, action = transition_workspace[\n",
    "            \"env/terminated\", \"env/reward\", \"action\"\n",
    "        ]\n",
    "        # print(f\"done {done}, reward {reward}, action {action}\")\n",
    "        \n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing\n",
    "        must_bootstrap = ~terminated[1]\n",
    "\n",
    "        # Critic update\n",
    "        # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n",
    "        q_agent(transition_workspace, t=0, n_steps=1)\n",
    "        # print(f\"q_values ante : {q_values}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute Q(s_{t+1}, \\pi(s_{t+1}) below\n",
    "            ag_actor(transition_workspace, t=1, n_steps=1)\n",
    "            # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n",
    "            target_q_agent(transition_workspace, t=1, n_steps=1)\n",
    "            # q_agent(rb_workspace, t=1, n_steps=1)\n",
    "        # finally q_values contains the above collection at t=0 and t=1\n",
    "\n",
    "        q_values, post_q_values = transition_workspace[\"critic/q_value\", \"target-critic/q_value\"]\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = compute_critic_loss(\n",
    "            cfg, reward, must_bootstrap, q_values[0], post_q_values[1]\n",
    "        )\n",
    "        logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            critic.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Actor update\n",
    "        choice = torch.rand(1)\n",
    "\n",
    "        # Now we determine the actions the current policy would take in the states from the RB\n",
    "        ag_actor(transition_workspace, t=0, n_steps=1)\n",
    "        # We determine the Q values resulting from actions of the current policy\n",
    "        q_agent(transition_workspace, t=0, n_steps=1)\n",
    "        # and we back-propagate the corresponding loss to maximize the Q values\n",
    "        q_values = transition_workspace[\"critic/q_value\"]\n",
    "        actor_loss = compute_actor_loss(q_values)\n",
    "        logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n",
    "        # if -25 < actor_loss < 0 and nb_steps > 2e5:\n",
    "        if choice > p:\n",
    "            # if -25 < actor_loss < 0 and nb_steps > 2e5:\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            actor_optimizer.step()\n",
    "        # decay not given in the article (we go to minimum of 0.01)\n",
    "        p*=0.999\n",
    "        p = max(p, 0.01)\n",
    "\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\")\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.add_log(\"reward\", mean, nb_steps)\n",
    "            pbar.set_description(f\"nb steps: {nb_steps}, reward: {mean:.3f}, best reward: {best_reward:.3f}\")\n",
    "\n",
    "            if mean > best_reward:\n",
    "                best_reward = mean\n",
    "\n",
    "                if cfg.save_best:\n",
    "                    directory = \"./ddpg_agent/\"\n",
    "                    if not os.path.exists(directory):\n",
    "                        os.makedirs(directory)\n",
    "                    filename = directory + \"ddpg_\" + str(mean.item()) + \".agt\"\n",
    "                    eval_agent.save_model(filename)\n",
    "\n",
    "                if False and cfg.plot_agents:\n",
    "                    plot_policy(\n",
    "                        actor,\n",
    "                        eval_env_agent,\n",
    "                        best_reward,\n",
    "                        \"\",\n",
    "                        cfg.gym_env.env_name,\n",
    "                    )\n",
    "                    plot_critic(\n",
    "                        q_agent.agent,\n",
    "                        eval_env_agent,\n",
    "                        best_reward,\n",
    "                        \"tmp\",\n",
    "                        cfg.gym_env.env_name,\n",
    "                        input_action=1, # input_action=None will not work for now\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1de154",
   "metadata": {},
   "source": [
    "# Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9940dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  # Set to true to have an insight on the learned policy\n",
    "  # (but slows down the evaluation a lot!)\n",
    "  \"plot_agents\": True,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/ddpg/ddpg-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 1,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 1,\n",
    "    \"n_steps\": 100,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"nb_evals\": 10,\n",
    "    \"max_epochs\": 21000,\n",
    "    \"discount_factor\": 0.98,\n",
    "    \"buffer_size\": 2e5,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_starts\": 10000,\n",
    "    \"action_noise\": 0.3,\n",
    "    \"architecture\":{\n",
    "        \"actor_hidden_size\": [12, 12],\n",
    "        \"critic_hidden_size\": [12, 12],\n",
    "        },\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPoleContinuous-v1\",\n",
    "  },\n",
    "  \"actor_optimizer\":{\n",
    "    \"classname\": \"torch.optim.SGD\",\n",
    "    \"lr\": 1e-3,\n",
    "  },\n",
    "  \"critic_optimizer\":{\n",
    "    \"classname\": \"torch.optim.SGD\",\n",
    "    \"lr\": 1e-3,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfa554",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f46e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to launch tensorboard in this notebook [y/n] y\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-32186c74094de207\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-32186c74094de207\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a142d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0e7b439a6b4d78bb7790bd7ed77096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "run_ddpg(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318abc2a",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Starting from the above version , you should code [the TD3\n",
    "algorithm](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf).\n",
    "\n",
    "For that, you need to use two critics (and two target critics) and always take\n",
    "the minimum output between the two when you ask for the Q-value of a (state,\n",
    "action) pair.\n",
    "\n",
    "In more detail, you have to do the following:\n",
    "- replace the single critic and corresponding target critic with two critics\n",
    "  and target critics (name them ```critic_1, critic_2, target_critic_1,\n",
    "  target_critic_2```)\n",
    "- get the q-values and target q-values corresponding to all these critics.\n",
    "- then the target q-values you should consider to update the critic should be\n",
    "  the min over the target q-values at each step (use ```torch.min(...)``` to\n",
    "  get this min over a sequence of data).\n",
    "- to update the actor, do it with the q-values of an arbitrarily chosen\n",
    "  critic, e.g. critic_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c73900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD3 algorithm\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafb23b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Experimental comparison\n",
    "\n",
    "Take an environment where the over-estimation bias may matter, and compare the performance of DDPG and TD3. Visualize the Q-value long before convergence to see whether indeed DDPG overestimates the Q-values with respect to TD3."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
