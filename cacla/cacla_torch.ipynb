{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2024-01-11 09:18:42 GMT\n",
      "OK.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import logger as gymlogger\n",
    "# from gym.wrappers import Monitor # deprecated 2023 - https://stackoverflow.com/questions/71520568/importerror-cannot-import-name-monitor-from-gym-wrappers\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"\\n\",date.today(), datetime.now().strftime(\"%H:%M:%S\"),\"GMT\") # timestamp is greenwich time\n",
    "print(\"OK.\")\n",
    "\n",
    "def show_video(loop=True, num=0):\n",
    "    mp4list = glob.glob(f'videoTest/rl-video-episode-{num}.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        if loop == True:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "        else:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './videoTest',  episode_trigger = lambda episode_number: True) # !!! 2023\n",
    "    env.reset() # !!! 2023\n",
    "    #env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "import sys\n",
    "sys.modules[__name__]\n",
    "\n",
    "class ContinuousEnvArticle(CartPoleEnv):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.action_space = spaces.Box(\n",
    "            self.min_action, self.max_action, shape=(1,), dtype=np.float64\n",
    "        )\n",
    "        self.tau = 0.1\n",
    "        \n",
    "        self.x_threshold = 1\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action > self.max_action:\n",
    "            action = np.array(self.max_action)\n",
    "        elif action < self.min_action:\n",
    "            action = np.array(self.min_action)\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag * float(action)\n",
    "\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "    \n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        x = self.np_random.uniform(low=-0.05, high=0.05, size=(1,))\n",
    "        self.state = 0, 0, x, 0\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "import sys\n",
    "sys.modules[__name__]\n",
    "\n",
    "class ContinuousEnvCACLA(CartPoleEnv):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.action_space = spaces.Box(\n",
    "            self.min_action, self.max_action, shape=(1,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        if action > self.max_action:\n",
    "            action = np.array(self.max_action)\n",
    "        elif action < self.min_action:\n",
    "            action = np.array(self.min_action)\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag * float(action)\n",
    "\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = -1\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvCacla',\n",
    "      entry_point='__main__:ContinuousEnvCACLA',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvArticle',\n",
    "      entry_point='__main__:ContinuousEnvArticle',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "class Cacla(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 h_dim, activation=nn.Tanh,\n",
    "                 discount_factor=0.95, \n",
    "                 gaussian_noise=0.01, \n",
    "                 var=False, \n",
    "                 exploration='gaussian'):\n",
    "        super(Cacla,self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "            #activation()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.vart = 1\n",
    "        self.beta = 0.001\n",
    "        self.with_var = var\n",
    "\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.99\n",
    "        self.eps_min = 0.01\n",
    "        \n",
    "        self.exploration = 'gaussian'\n",
    "    \n",
    "    def forward(self, x, testing=False):\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        if testing: return (None, self.actor(x)) \n",
    "        return self.critic(x), self.actor(x)\n",
    "    \n",
    "    # Loss computation\n",
    "    def compute_critic_loss(self, value, target):\n",
    "        delta = (target - value).detach()\n",
    "        loss =  mse_loss(value, target)\n",
    "        n_update = 1\n",
    "        if self.with_var and delta > 0:\n",
    "            n_update = torch.ceil(delta/np.sqrt(self.vart)).item()\n",
    "            self.vart = (1-self.beta)*self.vart +self.beta*loss.detach()\n",
    "        return loss, int(n_update)\n",
    "    \n",
    "    def compute_actor_loss(self, value, target):\n",
    "        return mse_loss(value, target)\n",
    "    \n",
    "    # Exploration\n",
    "    def sample(self, x):\n",
    "        if self.exploration == 'gaussian':\n",
    "            return self.sample_gaussian(x)\n",
    "        elif self.exploration == 'epsilon':\n",
    "            return self.sample_epsilon(x)\n",
    "        else: \n",
    "            print('Exploration unknown: ', self.exploration)       \n",
    "    \n",
    "    def sample_gaussian(self, x):\n",
    "        return (x + np.random.normal(0,self.gaussian_noise))\n",
    "    \n",
    "    def sample_epsilon(self, x, inf=-1, sup=1):\n",
    "        if torch.rand(1) > self.eps: return x\n",
    "        else: return np.random.rand() * (sup - inf) - inf\n",
    "\n",
    "\n",
    "def test(eval_env, model, n_test = 10, noise_std = 0.3):\n",
    "    cum_reward = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_test):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            obs0,_ = eval_env.reset()\n",
    "            while not done and not truncated:\n",
    "                _, a0 = model(obs0, testing=True)  \n",
    "                obs0, reward, done,truncated,_ = eval_env.step(a0) \n",
    "                cum_reward += reward\n",
    "    return cum_reward/n_test\n",
    "\n",
    "def addnoise(x, std):\n",
    "    return x + np.random.normal(0,std)\n",
    "    \n",
    "def train(eval_env, \n",
    "          train_env,\n",
    "          model,\n",
    "          optim, \n",
    "          eval_interval=1000, \n",
    "          step_max=100000, \n",
    "          n_test=10, \n",
    "          noise_std=0.3):\n",
    "    obs ,_ = train_env.reset()\n",
    "    scores = []\n",
    "    optim_critic, optim_actor = optim\n",
    "    # boucle d'apprentissage\n",
    "    for it in range(step_max):\n",
    "        v0, a0 = model(obs)\n",
    "        obs0 = obs\n",
    "        action = model.sample(a0).detach()\n",
    "        obs, reward, done, truncated,reste = train_env.step(addnoise(action, noise_std)) \n",
    "        \n",
    "        # Compute losses\n",
    "        with torch.no_grad(): v1, _ = model(obs)\n",
    "        target_v = addnoise(reward, noise_std) + model.discount_factor*v1\n",
    "        optim_critic.zero_grad()\n",
    "        critic_loss, n_update = model.compute_critic_loss(v0, target=target_v)\n",
    "        critic_loss.backward()\n",
    "        optim_critic.step()\n",
    "        if target_v - v0 > 0: \n",
    "            for _ in range(n_update):\n",
    "                optim_actor.zero_grad()\n",
    "                mse_loss(model(obs0)[1], action).backward()\n",
    "                optim_actor.step()\n",
    "        if done or truncated: obs,_ = train_env.reset()\n",
    "        if it % eval_interval == 0:\n",
    "            perf = test(eval_env, model, n_test, noise_std)\n",
    "            scores.append((it, perf))\n",
    "            print(f'{it = } | reward {perf}')\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= run = 1 / 20 =========\n",
      "it = 0 | reward 15.3\n",
      "it = 1024 | reward 16.0\n",
      "it = 2048 | reward 45.3\n",
      "it = 3072 | reward 50.2\n",
      "it = 4096 | reward 43.0\n",
      "it = 5120 | reward 46.9\n",
      "it = 6144 | reward 55.9\n",
      "it = 7168 | reward 35.2\n",
      "it = 8192 | reward 41.5\n",
      "it = 9216 | reward 43.6\n",
      "it = 10240 | reward 66.3\n",
      "it = 11264 | reward 41.8\n",
      "it = 12288 | reward 45.1\n",
      "it = 13312 | reward 77.2\n",
      "it = 14336 | reward 111.7\n",
      "it = 15360 | reward 42.3\n",
      "it = 16384 | reward 117.1\n",
      "it = 17408 | reward 138.4\n",
      "it = 18432 | reward 69.8\n",
      "it = 19456 | reward 117.4\n",
      "it = 20480 | reward 120.5\n",
      "it = 21504 | reward 57.5\n",
      "it = 22528 | reward 85.6\n",
      "it = 23552 | reward 61.6\n",
      "it = 24576 | reward 158.7\n",
      "it = 25600 | reward 88.7\n",
      "it = 26624 | reward 120.5\n",
      "it = 27648 | reward 110.6\n",
      "it = 28672 | reward 65.1\n",
      "it = 29696 | reward 141.4\n",
      "it = 30720 | reward 163.1\n",
      "it = 31744 | reward 66.2\n",
      "it = 32768 | reward 234.6\n",
      "it = 33792 | reward 128.8\n",
      "it = 34816 | reward 106.9\n",
      "it = 35840 | reward 31.3\n",
      "it = 36864 | reward 46.3\n",
      "it = 37888 | reward 52.7\n",
      "it = 38912 | reward 29.0\n",
      "it = 39936 | reward 24.4\n",
      "it = 40960 | reward 212.1\n",
      "it = 41984 | reward 153.5\n",
      "it = 43008 | reward 305.3\n",
      "it = 44032 | reward 247.8\n",
      "it = 45056 | reward 155.2\n",
      "it = 46080 | reward 250.4\n",
      "it = 47104 | reward 294.4\n",
      "it = 48128 | reward 201.9\n",
      "it = 49152 | reward 120.9\n",
      "it = 50176 | reward 152.0\n",
      "it = 51200 | reward 82.2\n",
      "it = 52224 | reward 310.6\n",
      "it = 53248 | reward 368.2\n",
      "it = 54272 | reward 186.8\n",
      "it = 55296 | reward 190.0\n",
      "it = 56320 | reward 183.3\n",
      "it = 57344 | reward 119.1\n",
      "it = 58368 | reward 190.1\n",
      "it = 59392 | reward 153.6\n",
      "it = 60416 | reward 203.3\n",
      "it = 61440 | reward 198.5\n",
      "it = 62464 | reward 285.7\n",
      "it = 63488 | reward 195.4\n",
      "it = 64512 | reward 147.4\n",
      "it = 65536 | reward 108.6\n",
      "it = 66560 | reward 84.3\n",
      "it = 67584 | reward 136.6\n",
      "it = 68608 | reward 166.3\n",
      "it = 69632 | reward 217.8\n",
      "it = 70656 | reward 500.0\n",
      "it = 71680 | reward 223.6\n",
      "it = 72704 | reward 171.8\n",
      "it = 73728 | reward 434.7\n",
      "it = 74752 | reward 500.0\n",
      "it = 75776 | reward 389.6\n",
      "it = 76800 | reward 392.5\n",
      "it = 77824 | reward 146.8\n",
      "it = 78848 | reward 201.9\n",
      "it = 79872 | reward 451.9\n",
      "it = 80896 | reward 316.0\n",
      "it = 81920 | reward 149.8\n",
      "it = 82944 | reward 151.8\n",
      "it = 83968 | reward 226.9\n",
      "it = 84992 | reward 85.8\n",
      "it = 86016 | reward 100.6\n",
      "it = 87040 | reward 15.8\n",
      "it = 88064 | reward 93.0\n",
      "it = 89088 | reward 160.2\n",
      "it = 90112 | reward 172.0\n",
      "it = 91136 | reward 318.9\n",
      "it = 92160 | reward 179.5\n",
      "it = 93184 | reward 256.8\n",
      "it = 94208 | reward 323.4\n",
      "it = 95232 | reward 135.5\n",
      "it = 96256 | reward 157.9\n",
      "it = 97280 | reward 96.5\n",
      "it = 98304 | reward 130.9\n",
      "it = 99328 | reward 148.5\n",
      "it = 100352 | reward 173.0\n",
      "it = 101376 | reward 142.9\n",
      "======= run = 2 / 20 =========\n",
      "it = 0 | reward 18.2\n",
      "it = 1024 | reward 36.0\n",
      "it = 2048 | reward 47.7\n",
      "it = 3072 | reward 47.1\n",
      "it = 4096 | reward 48.7\n",
      "it = 5120 | reward 65.5\n",
      "it = 6144 | reward 35.1\n",
      "it = 7168 | reward 43.5\n",
      "it = 8192 | reward 51.7\n",
      "it = 9216 | reward 52.4\n",
      "it = 10240 | reward 55.1\n",
      "it = 11264 | reward 58.3\n",
      "it = 12288 | reward 74.0\n",
      "it = 13312 | reward 61.9\n",
      "it = 14336 | reward 34.3\n",
      "it = 15360 | reward 71.0\n",
      "it = 16384 | reward 64.9\n",
      "it = 17408 | reward 72.6\n",
      "it = 18432 | reward 41.6\n",
      "it = 19456 | reward 44.4\n",
      "it = 20480 | reward 49.3\n",
      "it = 21504 | reward 88.7\n",
      "it = 22528 | reward 31.5\n",
      "it = 23552 | reward 56.3\n",
      "it = 24576 | reward 89.2\n",
      "it = 25600 | reward 67.2\n",
      "it = 26624 | reward 149.6\n",
      "it = 27648 | reward 76.2\n",
      "it = 28672 | reward 78.0\n",
      "it = 29696 | reward 115.1\n",
      "it = 30720 | reward 60.5\n",
      "it = 31744 | reward 173.7\n",
      "it = 32768 | reward 98.6\n",
      "it = 33792 | reward 58.3\n",
      "it = 34816 | reward 95.0\n",
      "it = 35840 | reward 64.8\n",
      "it = 36864 | reward 117.6\n",
      "it = 37888 | reward 125.2\n",
      "it = 38912 | reward 115.1\n",
      "it = 39936 | reward 85.4\n",
      "it = 40960 | reward 85.8\n",
      "it = 41984 | reward 81.0\n",
      "it = 43008 | reward 78.6\n",
      "it = 44032 | reward 155.0\n",
      "it = 45056 | reward 312.8\n",
      "it = 46080 | reward 366.4\n",
      "it = 47104 | reward 145.5\n",
      "it = 48128 | reward 110.4\n",
      "it = 49152 | reward 32.8\n",
      "it = 50176 | reward 41.5\n",
      "it = 51200 | reward 48.2\n",
      "it = 52224 | reward 121.6\n",
      "it = 53248 | reward 104.6\n",
      "it = 54272 | reward 278.3\n",
      "it = 55296 | reward 114.1\n",
      "it = 56320 | reward 269.1\n",
      "it = 57344 | reward 112.4\n",
      "it = 58368 | reward 272.0\n",
      "it = 59392 | reward 138.0\n",
      "it = 60416 | reward 207.1\n",
      "it = 61440 | reward 105.7\n",
      "it = 62464 | reward 20.7\n",
      "it = 63488 | reward 41.2\n",
      "it = 64512 | reward 45.1\n",
      "it = 65536 | reward 203.0\n",
      "it = 66560 | reward 164.3\n",
      "it = 67584 | reward 173.7\n",
      "it = 68608 | reward 124.0\n",
      "it = 69632 | reward 51.0\n",
      "it = 70656 | reward 158.7\n",
      "it = 71680 | reward 117.9\n",
      "it = 72704 | reward 119.3\n",
      "it = 73728 | reward 184.7\n",
      "it = 74752 | reward 109.0\n",
      "it = 75776 | reward 290.8\n",
      "it = 76800 | reward 127.4\n",
      "it = 77824 | reward 148.8\n",
      "it = 78848 | reward 183.8\n",
      "it = 79872 | reward 176.6\n",
      "it = 80896 | reward 217.1\n",
      "it = 81920 | reward 217.0\n",
      "it = 82944 | reward 221.7\n",
      "it = 83968 | reward 202.7\n",
      "it = 84992 | reward 149.1\n",
      "it = 86016 | reward 133.7\n",
      "it = 87040 | reward 496.7\n",
      "it = 88064 | reward 205.3\n",
      "it = 89088 | reward 191.2\n",
      "it = 90112 | reward 280.1\n",
      "it = 91136 | reward 500.0\n",
      "it = 92160 | reward 459.3\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "# Initialisation\n",
    "max_episode = 500\n",
    "def make_env(max_episode_steps= 500):\n",
    "    return TimeLimit(gymnasium.make('CartpoleEnvCacla'), max_episode_steps)\n",
    "\n",
    "train_env = make_env(max_episode_steps=max_episode)\n",
    "eval_env = make_env(max_episode_steps=max_episode)\n",
    "\n",
    "all_perfs = []\n",
    "n_runs = 18\n",
    "discount_factor = 0.9\n",
    "noise_std= 0.3\n",
    "exploration = 'gaussian'\n",
    "path = f'log/{exploration}/now_gamma{int(discount_factor*100)}_std{noise_std}'\n",
    "\n",
    "if not os.path.exists(path): \n",
    "    print()\n",
    "    os.mkdir(path)\n",
    "\n",
    "for run in range(1,n_runs+1):\n",
    "    print(f'======= {run = } / {n_runs} =========')\n",
    "    cacla = Cacla(in_dim=4, h_dim=12, \n",
    "                  discount_factor=discount_factor, \n",
    "                  gaussian_noise=0.1,\n",
    "                 var=True,\n",
    "                 exploration=exploration)\n",
    "    optim_crit = SGD(cacla.critic.parameters(), lr=0.01)\n",
    "    optim_act = SGD(cacla.actor.parameters(), lr=0.01)\n",
    "    optim = (optim_crit, optim_act)\n",
    "    scores = train(eval_env, train_env, cacla, optim, \n",
    "                   eval_interval=1024, \n",
    "                   step_max=102400,\n",
    "                  n_test=10,\n",
    "                  noise_std= noise_std)\n",
    "    scores = np.array(scores)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.savetxt(path+f'/{timestr}.log', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_env = wrap_env(TimeLimit(ContinuousEnvCACLA(render_mode='rgb_array'),max_episode_steps=500))\n",
    "observation, _ = rec_env.reset()\n",
    "maxEvaluations = 2\n",
    "evaluation = 0\n",
    "score = []\n",
    "total = 0\n",
    "while evaluation < maxEvaluations:\n",
    "    rec_env.render()\n",
    "    _, a0 = cacla(observation)  \n",
    "    action = a0\n",
    "    observation, reward, done, truncated, _ = rec_env.step(action) \n",
    "    total += reward\n",
    "    if done or truncated:\n",
    "      score.append(total)\n",
    "      total = 0\n",
    "      evaluation = evaluation + 1\n",
    "      observation, _ = rec_env.reset()\n",
    "rec_env.close()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(path):\n",
    "    X = []\n",
    "    for data_file in os.listdir(path):\n",
    "        filepath = path + data_file \n",
    "        X.append(np.loadtxt(filepath)[:,:1227])\n",
    "    return np.array(X)\n",
    "path = 'log/gaussian/gamma90/'\n",
    "X = get(path)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    plt.plot(x[:,0], x[:,1], alpha=0.1)\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mean(0)\n",
    "plt.plot(x[:,0], x[:,1])\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
