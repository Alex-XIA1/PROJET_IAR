{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2024-01-20 14:28:12 GMT\n",
      "OK.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import logger as gymlogger\n",
    "# from gym.wrappers import Monitor # deprecated 2023 - https://stackoverflow.com/questions/71520568/importerror-cannot-import-name-monitor-from-gym-wrappers\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"\\n\",date.today(), datetime.now().strftime(\"%H:%M:%S\"),\"GMT\") # timestamp is greenwich time\n",
    "print(\"OK.\")\n",
    "\n",
    "def show_video(loop=True, num=0):\n",
    "    mp4list = glob.glob(f'videoTest/rl-video-episode-{num}.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        if loop == True:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "        else:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './videoTest',  episode_trigger = lambda episode_number: True) # !!! 2023\n",
    "    env.reset() # !!! 2023\n",
    "    #env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "import sys\n",
    "sys.modules[__name__]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "\n",
    "class ContinuousEnvCACLA(CartPoleEnv):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.action_space = spaces.Box(\n",
    "            self.min_action, self.max_action, shape=(1,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        if action > self.max_action:\n",
    "            action = np.array(self.max_action)\n",
    "        elif action < self.min_action:\n",
    "            action = np.array(self.min_action)\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag * float(action)\n",
    "\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "class ContinuousEnvArticle(ContinuousEnvCACLA):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1\n",
    "        self.max_action = 1\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        x = self.np_random.uniform(low=-0.05, high=0.05, size=(1,))\n",
    "        self.state = 0, 0, x, 0\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "    \n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvCacla',\n",
    "      entry_point='__main__:ContinuousEnvCACLA',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvArticle',\n",
    "      entry_point='__main__:ContinuousEnvArticle',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "from copy import copy\n",
    "class Cacla(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 h_dim, activation=nn.Tanh,\n",
    "                 discount_factor=0.95, \n",
    "                 gaussian_noise=0.01, \n",
    "                 var=False, \n",
    "                 exploration='gaussian',\n",
    "                 transform= lambda x: x,\n",
    "                transform_critic = lambda x: x,\n",
    "                transform_actor =  lambda x: x):\n",
    "        super(Cacla,self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        # transform\n",
    "        self.transform = transform\n",
    "        self.transform_critic = transform_critic\n",
    "        self.transform_actor = transform_actor\n",
    "        # learning hyperparaters\n",
    "        self.discount_factor = discount_factor\n",
    "        self.vart = 1\n",
    "        self.beta = 0.001\n",
    "        self.with_var = var\n",
    "\n",
    "        # exploration parameters\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.99\n",
    "        self.eps_min = 0.01\n",
    "        self.exploration = 'gaussian'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "        return self.transform_critic(self.critic(x)), self.transform_actor(self.actor(x))\n",
    "    \n",
    "    def actor_forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        return self.transform_actor(self.actor(x))\n",
    "                            \n",
    "    def critic_forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        return self.transform_critic(self.critic(x))\n",
    "\n",
    "    # Loss computation\n",
    "    def compute_critic_loss(self, value, target):\n",
    "        delta = (target - value).detach()\n",
    "        loss =  mse_loss(value, target)\n",
    "        n_update = 1\n",
    "        if self.with_var and delta > 0:\n",
    "            n_update = torch.ceil(delta/np.sqrt(self.vart)).item()\n",
    "            self.vart = (1-self.beta)*self.vart +self.beta*loss.detach()\n",
    "        return loss, int(n_update)\n",
    "    \n",
    "    def compute_actor_loss(self, value, target):\n",
    "        return mse_loss(value, target)\n",
    "    \n",
    "    # Exploration\n",
    "    def sample(self, x):\n",
    "        if self.exploration == 'gaussian':\n",
    "            return self.sample_gaussian(x)\n",
    "        elif self.exploration == 'epsilon':\n",
    "            return self.sample_epsilon(x)\n",
    "        else: \n",
    "            print('Exploration unknown: ', self.exploration)       \n",
    "    \n",
    "    def sample_gaussian(self, x):\n",
    "        return (x + np.random.normal(0,self.gaussian_noise))\n",
    "    \n",
    "    def sample_epsilon(self, x, inf=-1, sup=1):\n",
    "        if torch.rand(1) > self.eps: return x\n",
    "        else: return np.random.rand() * (sup - inf) - inf\n",
    "\n",
    "\n",
    "def test(eval_env, model, n_test = 10, noise_std = 0.3):\n",
    "    cum_reward = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_test):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            obs0,_ = eval_env.reset()\n",
    "            while not done and not truncated:\n",
    "                a0 = model.actor_forward(obs0)  \n",
    "                obs0, reward, done,truncated,_ = eval_env.step(a0) \n",
    "                cum_reward += reward\n",
    "    return cum_reward/n_test\n",
    "\n",
    "def addnoise(x, std):\n",
    "    return x + np.random.normal(0,std)\n",
    "    \n",
    "def train(eval_env, \n",
    "          train_env,\n",
    "          model,\n",
    "          optim, \n",
    "          step_max=102400, \n",
    "          step_eval=1024, \n",
    "          n_test=10, \n",
    "          noise_std=0.3):\n",
    "    obs ,_ = train_env.reset()\n",
    "    scores = []\n",
    "    optim_critic, optim_actor = optim\n",
    "    count = 0\n",
    "    # boucle d'apprentissage\n",
    "    for it in range(step_max+1):\n",
    "        v0, a0 = model(obs)\n",
    "        obs0 = obs\n",
    "        action = model.sample(a0).detach()\n",
    "        act = torch.clip(action,-1.,1.)\n",
    "        obs, reward, done, truncated,reste = train_env.step(addnoise(act, noise_std))\n",
    "        \n",
    "        # Compute losses\n",
    "        with torch.no_grad(): v1 = model.critic_forward(obs)\n",
    "        target_v = addnoise(reward, noise_std) + model.discount_factor*v1\n",
    "        optim_critic.zero_grad()\n",
    "        critic_loss, n_update = model.compute_critic_loss(v0, target=target_v)\n",
    "        critic_loss.backward()\n",
    "        optim_critic.step()\n",
    "        if (target_v - v0).item() > 0: \n",
    "            for _ in range(n_update):\n",
    "                optim_actor.zero_grad()\n",
    "                mse_loss(model.actor_forward(obs0), action).backward()\n",
    "                optim_actor.step()\n",
    "        if done or truncated: obs,_ = train_env.reset()\n",
    "        if it % (100*(1<<count)) == 0:\n",
    "        #if it % 1024 == 0:\n",
    "            #if it % step_eval == 0:\n",
    "            count += 1\n",
    "            perf = test(eval_env, model, n_test, noise_std)\n",
    "            scores.append((it, perf))\n",
    "            print(f'{it = } | reward {perf}')\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= run = 1 / 20 =========\n",
      "it = 0 | reward 14.6\n",
      "it = 200 | reward 13.1\n",
      "it = 400 | reward 12.3\n",
      "it = 800 | reward 35.9\n",
      "it = 1600 | reward 50.8\n",
      "it = 3200 | reward 51.7\n",
      "it = 6400 | reward 34.9\n",
      "it = 12800 | reward 102.2\n",
      "it = 25600 | reward 237.0\n",
      "it = 51200 | reward 135.7\n",
      "it = 102400 | reward 165.2\n",
      "======= run = 2 / 20 =========\n",
      "it = 0 | reward 41.0\n",
      "it = 200 | reward 33.8\n",
      "it = 400 | reward 31.0\n",
      "it = 800 | reward 57.0\n",
      "it = 1600 | reward 63.7\n",
      "it = 3200 | reward 29.6\n",
      "it = 6400 | reward 38.6\n",
      "it = 12800 | reward 144.2\n",
      "it = 25600 | reward 275.3\n",
      "it = 51200 | reward 96.0\n",
      "it = 102400 | reward 192.2\n",
      "======= run = 3 / 20 =========\n",
      "it = 0 | reward 20.7\n",
      "it = 200 | reward 24.3\n",
      "it = 400 | reward 19.1\n",
      "it = 800 | reward 26.9\n",
      "it = 1600 | reward 19.3\n",
      "it = 3200 | reward 30.1\n",
      "it = 6400 | reward 42.7\n",
      "it = 12800 | reward 244.8\n",
      "it = 25600 | reward 33.2\n",
      "it = 51200 | reward 239.4\n",
      "it = 102400 | reward 452.6\n",
      "======= run = 4 / 20 =========\n",
      "it = 0 | reward 17.5\n",
      "it = 200 | reward 21.8\n",
      "it = 400 | reward 26.8\n",
      "it = 800 | reward 26.4\n",
      "it = 1600 | reward 29.0\n",
      "it = 3200 | reward 41.1\n",
      "it = 6400 | reward 38.5\n",
      "it = 12800 | reward 63.4\n",
      "it = 25600 | reward 116.0\n",
      "it = 51200 | reward 37.2\n",
      "it = 102400 | reward 119.6\n",
      "======= run = 5 / 20 =========\n",
      "it = 0 | reward 21.0\n",
      "it = 200 | reward 25.9\n",
      "it = 400 | reward 35.9\n",
      "it = 800 | reward 42.6\n",
      "it = 1600 | reward 28.3\n",
      "it = 3200 | reward 28.9\n",
      "it = 6400 | reward 104.9\n",
      "it = 12800 | reward 44.3\n",
      "it = 25600 | reward 182.6\n",
      "it = 51200 | reward 186.0\n",
      "it = 102400 | reward 183.5\n",
      "======= run = 6 / 20 =========\n",
      "it = 0 | reward 31.4\n",
      "it = 200 | reward 36.7\n",
      "it = 400 | reward 28.6\n",
      "it = 800 | reward 34.1\n",
      "it = 1600 | reward 25.9\n",
      "it = 3200 | reward 37.3\n",
      "it = 6400 | reward 80.2\n",
      "it = 12800 | reward 42.5\n",
      "it = 25600 | reward 266.7\n",
      "it = 51200 | reward 154.8\n",
      "it = 102400 | reward 132.2\n",
      "======= run = 7 / 20 =========\n",
      "it = 0 | reward 19.2\n",
      "it = 200 | reward 19.0\n",
      "it = 400 | reward 15.8\n",
      "it = 800 | reward 20.6\n",
      "it = 1600 | reward 52.8\n",
      "it = 3200 | reward 45.6\n",
      "it = 6400 | reward 52.9\n",
      "it = 12800 | reward 50.4\n",
      "it = 25600 | reward 326.6\n",
      "it = 51200 | reward 308.0\n",
      "it = 102400 | reward 288.4\n",
      "======= run = 8 / 20 =========\n",
      "it = 0 | reward 13.2\n",
      "it = 200 | reward 12.9\n",
      "it = 400 | reward 16.5\n",
      "it = 800 | reward 41.1\n",
      "it = 1600 | reward 44.5\n",
      "it = 3200 | reward 24.7\n",
      "it = 6400 | reward 45.7\n",
      "it = 12800 | reward 63.4\n",
      "it = 25600 | reward 197.3\n",
      "it = 51200 | reward 163.6\n",
      "it = 102400 | reward 161.2\n",
      "======= run = 9 / 20 =========\n",
      "it = 0 | reward 22.8\n",
      "it = 200 | reward 18.8\n",
      "it = 400 | reward 16.1\n",
      "it = 800 | reward 33.1\n",
      "it = 1600 | reward 47.8\n",
      "it = 3200 | reward 42.5\n",
      "it = 6400 | reward 56.1\n",
      "it = 12800 | reward 77.4\n",
      "it = 25600 | reward 299.5\n",
      "it = 51200 | reward 130.1\n",
      "it = 102400 | reward 445.7\n",
      "======= run = 10 / 20 =========\n",
      "it = 0 | reward 37.3\n",
      "it = 200 | reward 24.0\n",
      "it = 400 | reward 19.8\n",
      "it = 800 | reward 23.6\n",
      "it = 1600 | reward 21.9\n",
      "it = 3200 | reward 48.5\n",
      "it = 6400 | reward 48.5\n",
      "it = 12800 | reward 66.8\n",
      "it = 25600 | reward 403.2\n",
      "it = 51200 | reward 238.4\n",
      "it = 102400 | reward 181.0\n",
      "======= run = 11 / 20 =========\n",
      "it = 0 | reward 41.3\n",
      "it = 200 | reward 28.8\n",
      "it = 400 | reward 15.0\n",
      "it = 800 | reward 15.5\n",
      "it = 1600 | reward 38.0\n",
      "it = 3200 | reward 42.8\n",
      "it = 6400 | reward 59.3\n",
      "it = 12800 | reward 134.6\n",
      "it = 25600 | reward 205.0\n",
      "it = 51200 | reward 127.5\n",
      "it = 102400 | reward 188.0\n",
      "======= run = 12 / 20 =========\n",
      "it = 0 | reward 42.8\n",
      "it = 200 | reward 30.3\n",
      "it = 400 | reward 37.0\n",
      "it = 800 | reward 30.3\n",
      "it = 1600 | reward 54.8\n",
      "it = 3200 | reward 56.2\n",
      "it = 6400 | reward 57.8\n",
      "it = 12800 | reward 46.4\n",
      "it = 25600 | reward 150.6\n",
      "it = 51200 | reward 301.0\n",
      "it = 102400 | reward 185.6\n",
      "======= run = 13 / 20 =========\n",
      "it = 0 | reward 12.4\n",
      "it = 200 | reward 10.1\n",
      "it = 400 | reward 13.6\n",
      "it = 800 | reward 36.6\n",
      "it = 1600 | reward 38.7\n",
      "it = 3200 | reward 49.8\n",
      "it = 6400 | reward 62.7\n",
      "it = 12800 | reward 57.7\n",
      "it = 25600 | reward 49.3\n",
      "it = 51200 | reward 129.5\n",
      "it = 102400 | reward 140.7\n",
      "======= run = 14 / 20 =========\n",
      "it = 0 | reward 32.3\n",
      "it = 200 | reward 28.3\n",
      "it = 400 | reward 17.0\n",
      "it = 800 | reward 27.1\n",
      "it = 1600 | reward 36.1\n",
      "it = 3200 | reward 51.3\n",
      "it = 6400 | reward 54.2\n",
      "it = 12800 | reward 41.5\n",
      "it = 25600 | reward 99.1\n",
      "it = 51200 | reward 137.8\n",
      "it = 102400 | reward 151.4\n",
      "======= run = 15 / 20 =========\n",
      "it = 0 | reward 19.0\n",
      "it = 200 | reward 20.2\n",
      "it = 400 | reward 14.1\n",
      "it = 800 | reward 15.9\n",
      "it = 1600 | reward 21.3\n",
      "it = 3200 | reward 36.7\n",
      "it = 6400 | reward 48.9\n",
      "it = 12800 | reward 108.5\n",
      "it = 25600 | reward 228.8\n",
      "it = 51200 | reward 298.7\n",
      "it = 102400 | reward 95.0\n",
      "======= run = 16 / 20 =========\n",
      "it = 0 | reward 42.6\n",
      "it = 200 | reward 24.1\n",
      "it = 400 | reward 13.2\n",
      "it = 800 | reward 40.4\n",
      "it = 1600 | reward 60.8\n",
      "it = 3200 | reward 41.3\n",
      "it = 6400 | reward 35.5\n",
      "it = 12800 | reward 46.6\n",
      "it = 25600 | reward 123.6\n",
      "it = 51200 | reward 259.3\n",
      "it = 102400 | reward 110.7\n",
      "======= run = 17 / 20 =========\n",
      "it = 0 | reward 11.2\n",
      "it = 200 | reward 11.5\n",
      "it = 400 | reward 11.7\n",
      "it = 800 | reward 30.6\n",
      "it = 1600 | reward 35.9\n",
      "it = 3200 | reward 54.2\n",
      "it = 6400 | reward 49.6\n",
      "it = 12800 | reward 51.3\n",
      "it = 25600 | reward 246.9\n",
      "it = 51200 | reward 144.4\n",
      "it = 102400 | reward 144.1\n",
      "======= run = 18 / 20 =========\n",
      "it = 0 | reward 21.2\n",
      "it = 200 | reward 18.8\n",
      "it = 400 | reward 14.7\n",
      "it = 800 | reward 12.7\n",
      "it = 1600 | reward 24.4\n",
      "it = 3200 | reward 34.2\n",
      "it = 6400 | reward 58.8\n",
      "it = 12800 | reward 96.6\n",
      "it = 25600 | reward 251.7\n",
      "it = 51200 | reward 247.1\n",
      "it = 102400 | reward 36.0\n",
      "======= run = 19 / 20 =========\n",
      "it = 0 | reward 23.2\n",
      "it = 200 | reward 14.4\n",
      "it = 400 | reward 14.3\n",
      "it = 800 | reward 17.1\n",
      "it = 1600 | reward 29.9\n",
      "it = 3200 | reward 60.7\n",
      "it = 6400 | reward 76.1\n",
      "it = 12800 | reward 32.8\n",
      "it = 25600 | reward 242.2\n",
      "it = 51200 | reward 276.7\n",
      "it = 102400 | reward 117.5\n",
      "======= run = 20 / 20 =========\n",
      "it = 0 | reward 20.6\n",
      "it = 200 | reward 18.4\n",
      "it = 400 | reward 22.6\n",
      "it = 800 | reward 20.4\n",
      "it = 1600 | reward 26.5\n",
      "it = 3200 | reward 44.2\n",
      "it = 6400 | reward 57.2\n",
      "it = 12800 | reward 93.6\n",
      "it = 25600 | reward 291.9\n",
      "it = 51200 | reward 177.9\n",
      "it = 102400 | reward 186.8\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from functools import partial\n",
    "# Initialisation\n",
    "\n",
    "def make_env(env_name, max_episode_steps= 500):\n",
    "    return TimeLimit(gymnasium.make(env_name), max_episode_steps)\n",
    "\n",
    "# Make env\n",
    "env_name = 'CartpoleEnvArticle'\n",
    "#env_name = 'CartpoleEnvArticle'\n",
    "max_episode = 500\n",
    "train_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "eval_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "\n",
    "all_perfs = []\n",
    "n_runs = 20\n",
    "\n",
    "# Model hyperparameters\n",
    "discount_factor = 0.90\n",
    "noise_std= 0.3\n",
    "exploration = 'epsilon'\n",
    "var = True\n",
    "varname = 'var' if var else 'novar'\n",
    "path = f'log/{exploration}/32_{varname}_normobs_gamma{int(discount_factor*100)}_std{noise_std}'\n",
    "\n",
    "obsmax = np.array([0.21, 2.14, 0.27, 3.32], dtype=np.float32)\n",
    "vmax, vmin = 5, 5\n",
    "amax, amin = -1, 1\n",
    "\n",
    "def norm_obs(x, xmax):\n",
    "    return x / xmax\n",
    "\n",
    "def norm_out(v, vmin, vmax):\n",
    "    return 2*(v - vmin) / (vmax-vmax)-1\n",
    "\n",
    "if not os.path.exists(path): \n",
    "    os.mkdir(path)\n",
    "\n",
    "for run in range(1,n_runs+1):\n",
    "    print(f'======= {run = } / {n_runs} =========')\n",
    "    cacla = Cacla(in_dim=4, h_dim=32, \n",
    "                  discount_factor=discount_factor, \n",
    "                  gaussian_noise=0.1,\n",
    "                  activation = nn.Tanh,\n",
    "                 var=var,\n",
    "                 exploration=exploration,\n",
    "                 #transform=partial(norm_obs, xmax=obsmax),\n",
    "                 #transform_critic = partial(norm_out, vmin=vmin, vmax=vmax),\n",
    "                 #transform_actor = partial(norm_out, vmin=amin, vmax=amax)\n",
    "                 )\n",
    "    optim_crit = SGD(cacla.critic.parameters(), lr=0.01)\n",
    "    optim_act = SGD(cacla.actor.parameters(), lr=0.01)\n",
    "    optim = (optim_crit, optim_act)\n",
    "    scores = train(eval_env, train_env, cacla, optim, \n",
    "                   step_max=102400,\n",
    "                  n_test=10,\n",
    "                  noise_std= noise_std)\n",
    "    scores = np.array(scores)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.savetxt(path+f'/{timestr}.log', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_env = wrap_env(TimeLimit(ContinuousEnvCACLA(render_mode='rgb_array'),max_episode_steps=500))\n",
    "observation, _ = rec_env.reset()\n",
    "maxEvaluations = 2\n",
    "evaluation = 0\n",
    "score = []\n",
    "total = 0\n",
    "while evaluation < maxEvaluations:\n",
    "    rec_env.render()\n",
    "    _, a0 = cacla(observation)  \n",
    "    action = a0\n",
    "    observation, reward, done, truncated, _ = rec_env.step(action) \n",
    "    total += reward\n",
    "    if done or truncated:\n",
    "      score.append(total)\n",
    "      total = 0\n",
    "      evaluation = evaluation + 1\n",
    "      observation, _ = rec_env.reset()\n",
    "rec_env.close()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get(path):\n",
    "    X = []\n",
    "    for data_file in os.listdir(path):\n",
    "        filepath = path + data_file \n",
    "        X.append(np.loadtxt(filepath)[:,:1227])\n",
    "    return np.array(X)\n",
    "#path = 'log/gaussian/gamma90/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CACLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get('./log/gaussian/32_novar_normobs_gamma95_std0.3/')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    plt.plot(x[:,0], x[:,1], alpha=1)\n",
    "    #break\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mean(0)\n",
    "plt.plot(x[:,0], x[:,1])\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CACLA + VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get('./log/gaussian/now_gamma90_std0.3/')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[:5]:\n",
    "    plt.plot(x[:,0], x[:,1], alpha=1)\n",
    "    #break\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mean(0)\n",
    "plt.plot(x[:,0], x[:,1])\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
