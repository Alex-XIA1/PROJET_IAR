{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2024-01-17 14:08:22 GMT\n",
      "OK.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import logger as gymlogger\n",
    "# from gym.wrappers import Monitor # deprecated 2023 - https://stackoverflow.com/questions/71520568/importerror-cannot-import-name-monitor-from-gym-wrappers\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"\\n\",date.today(), datetime.now().strftime(\"%H:%M:%S\"),\"GMT\") # timestamp is greenwich time\n",
    "print(\"OK.\")\n",
    "\n",
    "def show_video(loop=True, num=0):\n",
    "    mp4list = glob.glob(f'videoTest/rl-video-episode-{num}.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        if loop == True:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "        else:\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:videoTest/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './videoTest',  episode_trigger = lambda episode_number: True) # !!! 2023\n",
    "    env.reset() # !!! 2023\n",
    "    #env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "import sys\n",
    "sys.modules[__name__]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Continuous action version of the classic cart-pole system implemented by Rich\n",
    "Sutton et al.\n",
    "\"\"\"\n",
    "\n",
    "class ContinuousEnvCACLA(CartPoleEnv):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.action_space = spaces.Box(\n",
    "            self.min_action, self.max_action, shape=(1,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        if action > self.max_action:\n",
    "            action = np.array(self.max_action)\n",
    "        elif action < self.min_action:\n",
    "            action = np.array(self.min_action)\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag * float(action)\n",
    "\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "class ContinuousEnvArticle(ContinuousEnvCACLA):\n",
    "    \"\"\"Continuous version  of the CartPole-v1 environment\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_action = -1\n",
    "        self.max_action = 1\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        x = self.np_random.uniform(low=-0.05, high=0.05, size=(1,))\n",
    "        self.state = 0, 0, x, 0\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "    \n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvCacla',\n",
    "      entry_point='__main__:ContinuousEnvCACLA',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n",
    "try:\n",
    "  gymnasium.envs.register(\n",
    "      id='CartpoleEnvArticle',\n",
    "      entry_point='__main__:ContinuousEnvArticle',\n",
    "      max_episode_steps=500\n",
    "  )\n",
    "except:\n",
    "    print(\"Except\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "from copy import copy\n",
    "class Cacla(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 h_dim, activation=nn.Tanh,\n",
    "                 discount_factor=0.95, \n",
    "                 gaussian_noise=0.01, \n",
    "                 var=False, \n",
    "                 exploration='gaussian',\n",
    "                 transform= lambda x: x,\n",
    "                transform_critic = lambda x: x,\n",
    "                transform_actor =  lambda x: x):\n",
    "        super(Cacla,self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        # transform\n",
    "        self.transform = transform\n",
    "        self.transform_critic = transform_critic\n",
    "        self.transform_actor = transform_actor\n",
    "        # learning hyperparaters\n",
    "        self.discount_factor = discount_factor\n",
    "        self.vart = 1\n",
    "        self.beta = 0.001\n",
    "        self.with_var = var\n",
    "\n",
    "        # exploration parameters\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.99\n",
    "        self.eps_min = 0.01\n",
    "        self.exploration = 'gaussian'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "        return self.transform_critic(self.critic(x)), self.transform_actor(self.actor(x))\n",
    "    \n",
    "    def actor_forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        return self.transform_actor(self.actor(x))\n",
    "                            \n",
    "    def critic_forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        return self.transform_critic(self.critic(x))\n",
    "\n",
    "    # Loss computation\n",
    "    def compute_critic_loss(self, value, target):\n",
    "        delta = (target - value).detach()\n",
    "        loss =  mse_loss(value, target)\n",
    "        n_update = 1\n",
    "        if self.with_var and delta > 0:\n",
    "            n_update = torch.ceil(delta/np.sqrt(self.vart)).item()\n",
    "            self.vart = (1-self.beta)*self.vart +self.beta*loss.detach()\n",
    "        return loss, int(n_update)\n",
    "    \n",
    "    def compute_actor_loss(self, value, target):\n",
    "        return mse_loss(value, target)\n",
    "    \n",
    "    # Exploration\n",
    "    def sample(self, x):\n",
    "        if self.exploration == 'gaussian':\n",
    "            return self.sample_gaussian(x)\n",
    "        elif self.exploration == 'epsilon':\n",
    "            return self.sample_epsilon(x)\n",
    "        else: \n",
    "            print('Exploration unknown: ', self.exploration)       \n",
    "    \n",
    "    def sample_gaussian(self, x):\n",
    "        return (x + np.random.normal(0,self.gaussian_noise))\n",
    "    \n",
    "    def sample_epsilon(self, x, inf=-1, sup=1):\n",
    "        if torch.rand(1) > self.eps: return x\n",
    "        else: return np.random.rand() * (sup - inf) - inf\n",
    "\n",
    "\n",
    "def test(eval_env, model, n_test = 10, noise_std = 0.3):\n",
    "    cum_reward = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_test):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            obs0,_ = eval_env.reset()\n",
    "            while not done and not truncated:\n",
    "                a0 = model.actor_forward(obs0)  \n",
    "                obs0, reward, done,truncated,_ = eval_env.step(a0) \n",
    "                cum_reward += reward\n",
    "    return cum_reward/n_test\n",
    "\n",
    "def addnoise(x, std):\n",
    "    return x + np.random.normal(0,std)\n",
    "    \n",
    "def train(eval_env, \n",
    "          train_env,\n",
    "          model,\n",
    "          optim, \n",
    "          step_max=102400, \n",
    "          step_eval=1024, \n",
    "          n_test=10, \n",
    "          noise_std=0.3):\n",
    "    obs ,_ = train_env.reset()\n",
    "    scores = []\n",
    "    optim_critic, optim_actor = optim\n",
    "    count = 0\n",
    "    # boucle d'apprentissage\n",
    "    for it in range(step_max+1):\n",
    "        v0, a0 = model(obs)\n",
    "        obs0 = obs\n",
    "        action = copy(a0)\n",
    "        action = model.sample(action).detach()\n",
    "        obs, reward, done, truncated, reste = train_env.step(action)\n",
    "        \n",
    "        # Compute losses\n",
    "        with torch.no_grad(): v1 = model.critic_forward(obs)\n",
    "        target_v = addnoise(reward, noise_std) + model.discount_factor*v1\n",
    "        optim_critic.zero_grad()\n",
    "        critic_loss, n_update = model.compute_critic_loss(v0, target=target_v)\n",
    "        critic_loss.backward()\n",
    "        optim_critic.step()\n",
    "        if (target_v - v0).item() > 0: \n",
    "            for _ in range(n_update):\n",
    "                optim_actor.zero_grad()\n",
    "                mse_loss(model.actor_forward(obs), action).backward()\n",
    "                optim_actor.step()\n",
    "        if done or truncated: obs,_ = train_env.reset()\n",
    "        #if it % (100*(1<<count)) == 0:\n",
    "        if it % 1024 == 0:\n",
    "            #if it % step_eval == 0:\n",
    "            count += 1\n",
    "            perf = test(eval_env, model, n_test, noise_std)\n",
    "            scores.append((it, perf))\n",
    "            print(f'{it = } | reward {perf}')\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= run = 1 / 20 =========\n",
      "it = 0 | reward 30.3\n",
      "it = 1024 | reward 18.2\n",
      "it = 2048 | reward 24.3\n",
      "it = 3072 | reward 43.1\n",
      "it = 4096 | reward 62.6\n",
      "it = 5120 | reward 71.7\n",
      "it = 6144 | reward 55.6\n",
      "it = 7168 | reward 83.3\n",
      "it = 8192 | reward 59.8\n",
      "it = 9216 | reward 61.9\n",
      "it = 10240 | reward 62.8\n",
      "it = 11264 | reward 54.0\n",
      "it = 12288 | reward 72.0\n",
      "it = 13312 | reward 70.4\n",
      "it = 14336 | reward 74.5\n",
      "it = 15360 | reward 84.8\n",
      "it = 16384 | reward 89.6\n",
      "it = 17408 | reward 81.9\n",
      "it = 18432 | reward 64.1\n",
      "it = 19456 | reward 77.0\n",
      "it = 20480 | reward 66.9\n",
      "it = 21504 | reward 69.9\n",
      "it = 22528 | reward 69.8\n",
      "it = 23552 | reward 86.9\n",
      "it = 24576 | reward 53.6\n",
      "it = 25600 | reward 63.9\n",
      "it = 26624 | reward 54.8\n",
      "it = 27648 | reward 70.7\n",
      "it = 28672 | reward 81.5\n",
      "it = 29696 | reward 68.8\n",
      "it = 30720 | reward 88.2\n",
      "it = 31744 | reward 54.9\n",
      "it = 32768 | reward 42.4\n",
      "it = 33792 | reward 58.0\n",
      "it = 34816 | reward 60.9\n",
      "it = 35840 | reward 100.9\n",
      "it = 36864 | reward 89.6\n",
      "it = 37888 | reward 45.6\n",
      "it = 38912 | reward 74.7\n",
      "it = 39936 | reward 73.1\n",
      "it = 40960 | reward 80.4\n",
      "it = 41984 | reward 80.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m optim_act \u001b[38;5;241m=\u001b[39m SGD(cacla\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     53\u001b[0m optim \u001b[38;5;241m=\u001b[39m (optim_crit, optim_act)\n\u001b[0;32m---> 54\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcacla\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstep_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m102400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnoise_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[1;32m     59\u001b[0m timestr \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [27], line 127\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(eval_env, train_env, model, optim, step_max, step_eval, n_test, noise_std)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): v1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcritic_forward(obs)\n\u001b[1;32m    126\u001b[0m target_v \u001b[38;5;241m=\u001b[39m addnoise(reward, noise_std) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mdiscount_factor\u001b[38;5;241m*\u001b[39mv1\n\u001b[0;32m--> 127\u001b[0m \u001b[43moptim_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m critic_loss, n_update \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_critic_loss(v0, target\u001b[38;5;241m=\u001b[39mtarget_v)\n\u001b[1;32m    129\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/users/nfs/Enseignants/piwowarski/venv/deepdac/lib/python3.9/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/users/nfs/Enseignants/piwowarski/venv/deepdac/lib/python3.9/site-packages/torch/_dynamo/decorators.py:47\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m callable(fn)\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/users/nfs/Enseignants/piwowarski/venv/deepdac/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:290\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(fn)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/inspect.py:706\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    704\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from functools import partial\n",
    "# Initialisation\n",
    "\n",
    "def make_env(env_name, max_episode_steps= 500):\n",
    "    return TimeLimit(gymnasium.make(env_name), max_episode_steps)\n",
    "\n",
    "# Make env\n",
    "env_name = 'CartpoleEnvArticle'\n",
    "#env_name = 'CartpoleEnvArticle'\n",
    "max_episode = 500\n",
    "train_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "eval_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "\n",
    "all_perfs = []\n",
    "n_runs = 20\n",
    "\n",
    "# Model hyperparameters\n",
    "discount_factor = 0.95\n",
    "noise_std= 0.3\n",
    "exploration = 'gaussian'\n",
    "var = False\n",
    "\n",
    "path = f'log/{exploration}/32_var_normobs_gamma{int(discount_factor*100)}_std{noise_std}'\n",
    "\n",
    "obsmax = np.array([0.21, 2.14, 0.27, 3.32], dtype=np.float32)\n",
    "vmax, vmin = 5, 5\n",
    "amax, amin = -1, 1\n",
    "\n",
    "def norm_obs(x, xmax):\n",
    "    return x / xmax\n",
    "\n",
    "def norm_out(v, vmin, vmax):\n",
    "    return 2*(v - vmin) / (vmax-vmax)-1\n",
    "\n",
    "if not os.path.exists(path): \n",
    "    os.mkdir(path)\n",
    "\n",
    "for run in range(1,n_runs+1):\n",
    "    print(f'======= {run = } / {n_runs} =========')\n",
    "    cacla = Cacla(in_dim=4, h_dim=32, \n",
    "                  discount_factor=discount_factor, \n",
    "                  gaussian_noise=0.1,\n",
    "                  activation = nn.Tanh,\n",
    "                 var=var,\n",
    "                 exploration=exploration,\n",
    "                 #transform=partial(norm_obs, xmax=obsmax),\n",
    "                 #transform_critic = partial(norm_out, vmin=vmin, vmax=vmax),\n",
    "                 #transform_actor = partial(norm_out, vmin=amin, vmax=amax)\n",
    "                 )\n",
    "    optim_crit = SGD(cacla.critic.parameters(), lr=0.01)\n",
    "    optim_act = SGD(cacla.actor.parameters(), lr=0.01)\n",
    "    optim = (optim_crit, optim_act)\n",
    "    scores = train(eval_env, train_env, cacla, optim, \n",
    "                   step_max=102400,\n",
    "                  n_test=10,\n",
    "                  noise_std= noise_std)\n",
    "    scores = np.array(scores)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.savetxt(path+f'/{timestr}.log', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention, ce sont des bornes empiriques\n",
    "print(f'Les bornes actions sont {cacla.actbornes} et les bornes etat sont {cacla.bornes} et bornes critic{cacla.vbornes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_env = wrap_env(TimeLimit(ContinuousEnvCACLA(render_mode='rgb_array'),max_episode_steps=500))\n",
    "observation, _ = rec_env.reset()\n",
    "maxEvaluations = 2\n",
    "evaluation = 0\n",
    "score = []\n",
    "total = 0\n",
    "while evaluation < maxEvaluations:\n",
    "    rec_env.render()\n",
    "    _, a0 = cacla(observation)  \n",
    "    action = a0\n",
    "    observation, reward, done, truncated, _ = rec_env.step(action) \n",
    "    total += reward\n",
    "    if done or truncated:\n",
    "      score.append(total)\n",
    "      total = 0\n",
    "      evaluation = evaluation + 1\n",
    "      observation, _ = rec_env.reset()\n",
    "rec_env.close()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get(path):\n",
    "    X = []\n",
    "    for data_file in os.listdir(path):\n",
    "        filepath = path + data_file \n",
    "        X.append(np.loadtxt(filepath)[:,:1227])\n",
    "    return np.array(X)\n",
    "#path = 'log/gaussian/gamma90/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CACLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get('./log/gaussian/now_novar_gamma90_std0.3/')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    plt.plot(x[:,0], x[:,1], alpha=1)\n",
    "    #break\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mean(0)\n",
    "plt.plot(x[:,0], x[:,1])\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CACLA + VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get('./log/gaussian/now_gamma90_std0.3/')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[:5]:\n",
    "    plt.plot(x[:,0], x[:,1], alpha=1)\n",
    "    #break\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = X.mean(0)\n",
    "plt.plot(x[:,0], x[:,1])\n",
    "plt.title(f'CACLA+Var: gamme: {discount_factor} | noise_std: {noise_std}')\n",
    "plt.xlabel('training iterations')\n",
    "plt.ylabel('reward cumulée')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import mse_loss\n",
    "class Cacla(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 h_dim, activation=nn.Tanh,\n",
    "                 discount_factor=0.95, \n",
    "                 gaussian_noise=0.01, \n",
    "                 var=False, \n",
    "                 exploration='gaussian'):\n",
    "        super(Cacla,self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "            #activation()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            activation(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.vart = 1\n",
    "        self.beta = 0.001\n",
    "        self.with_var = var\n",
    "\n",
    "        self.eps = 1\n",
    "        self.eps_decay = 0.99\n",
    "        self.eps_min = 0.01\n",
    "        \n",
    "        self.exploration = 'gaussian'\n",
    "        # ordre : xmax, xmin, xprimemax, xprimemin, thetamax, thetamin, thetaprimemax, thetaprimemin\n",
    "        self.bornes = np.array([np.inf for i in range(8)])\n",
    "        self.actbornes = np.array([np.inf for i in range(2)])\n",
    "        self.vbornes = np.array([np.inf,np.inf])\n",
    "    \n",
    "    def forward(self, x, testing=False):\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        if testing: return (None, self.actor(x)) \n",
    "        \n",
    "        res = self.critic(x), self.actor(x)\n",
    "        return res\n",
    "    \n",
    "    # Loss computation\n",
    "    def compute_critic_loss(self, value, target):\n",
    "        delta = (target - value).detach()\n",
    "        loss =  mse_loss(value, target)\n",
    "        n_update = 1\n",
    "        if self.with_var and delta > 0:\n",
    "            n_update = torch.ceil(delta/np.sqrt(self.vart)).item()\n",
    "            self.vart = (1-self.beta)*self.vart +self.beta*loss.detach()\n",
    "        return loss, int(n_update)\n",
    "    \n",
    "    def compute_actor_loss(self, value, target):\n",
    "        return mse_loss(value, target)\n",
    "    \n",
    "    # Exploration\n",
    "    def sample(self, x):\n",
    "        if self.exploration == 'gaussian':\n",
    "            return self.sample_gaussian(x)\n",
    "        elif self.exploration == 'epsilon':\n",
    "            return self.sample_epsilon(x)\n",
    "        else: \n",
    "            print('Exploration unknown: ', self.exploration)       \n",
    "    \n",
    "    def sample_gaussian(self, x):\n",
    "        return (x + np.random.normal(0,self.gaussian_noise))\n",
    "    \n",
    "    def sample_epsilon(self, x, inf=-1, sup=1):\n",
    "        if torch.rand(1) > self.eps: return x\n",
    "        else: return np.random.rand() * (sup - inf) - inf\n",
    "        \n",
    "    # Mise à jour des facteurs pour la normalisation\n",
    "    def updateB(self,x,theta,xprime,thetaprime):\n",
    "        # Le cas initial\n",
    "        if self.bornes[0] == np.inf :\n",
    "            self.bornes[0] = x\n",
    "            self.bornes[1] = x\n",
    "            self.bornes[2] = xprime\n",
    "            self.bornes[3] = xprime\n",
    "            self.bornes[4] = theta\n",
    "            self.bornes[5] = theta\n",
    "            self.bornes[6] = thetaprime\n",
    "            self.bornes[7] = thetaprime\n",
    "        \n",
    "        # update x, xprime, theta et thetaprime\n",
    "        self.bornes[0] = np.max((self.bornes[0],x))\n",
    "        self.bornes[1] = np.min((self.bornes[1],x))\n",
    "        \n",
    "        self.bornes[2] = np.max((self.bornes[2],xprime))\n",
    "        self.bornes[3] = np.min((self.bornes[3],xprime))\n",
    "        \n",
    "        self.bornes[4] = np.max((self.bornes[4],theta))\n",
    "        self.bornes[5] = np.min((self.bornes[5],theta))\n",
    "        \n",
    "        self.bornes[6] = np.max((self.bornes[6],thetaprime))\n",
    "        self.bornes[7] = np.min((self.bornes[7],thetaprime))\n",
    "\n",
    "def makenorm():\n",
    "    env_name = 'CartpoleEnvCacla'\n",
    "    num_samples = 1000\n",
    "    env = make_env(env_name)\n",
    "    samples = np.array([env.reset()[0] for _ in range(num_samples)])\n",
    "    min_values = np.min(samples, axis=0)\n",
    "    max_values = np.max(samples, axis=0)\n",
    "    return min_values, max_values\n",
    "\n",
    "def addnoise(x, std):\n",
    "    return x + np.random.normal(0,std)\n",
    "\n",
    "def normalize(x):\n",
    "    normact = 2*((x+0.77478176)/(0.85380614 + 0.77478176))-1\n",
    "    #print(\"Norm act \",normact)\n",
    "    return normact\n",
    "\n",
    "obsmax = np.array([0.21, 2.14, 0.27, 3.32], dtype=np.float32)\n",
    "bornes = np.array([2.45187998, -2.45674968,  \n",
    "                   0.23211133, -0.23395817,  \n",
    "                   3.21429443, -3.3032372, \n",
    "                   1.78533256, -1.56042111])\n",
    "def normalize_obs(x):\n",
    "    return x/obsmax\n",
    "\n",
    "def normalize_critic(x):\n",
    "    return 2*((x+0.19350152)/(11.54955292 + 0.19350152))-1\n",
    "    \n",
    "\n",
    "def test(eval_env, model, n_test = 10, noise_std = 0.3):\n",
    "    cum_reward = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_test):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            obs0,_ = eval_env.reset()\n",
    "            while not done and not truncated:\n",
    "                # normalisation\n",
    "                obs0 = normalize_obs(obs0)\n",
    "                _, a0 = model(obs0, testing=True)\n",
    "                \n",
    "                obs0, reward, done,truncated,_ = eval_env.step(normalize(a0))\n",
    "                \n",
    "                cum_reward += reward\n",
    "    return cum_reward/n_test\n",
    "    \n",
    "def train(eval_env, \n",
    "          train_env,\n",
    "          model,\n",
    "          optim, \n",
    "          eval_interval=1000, \n",
    "          step_max=100000, \n",
    "          n_test=10, \n",
    "          noise_std=0.3):\n",
    "    obs ,_ = train_env.reset()\n",
    "\n",
    "    \n",
    "    model.updateB(obs[0],obs[1],obs[2],obs[3])\n",
    "    scores = []\n",
    "    optim_critic, optim_actor = optim\n",
    "    # Extra pour récupérer des bornes\n",
    "    # boucle d'apprentissage\n",
    "    for it in range(step_max):\n",
    "        # quelques normalisations lineaires\n",
    "        obs = normalize_obs(obs)\n",
    "        v0, a0 = model(obs)\n",
    "        a0 = normalize(a0)\n",
    "        #v0 = normalize_critic(v0)\n",
    "\n",
    "        obs0 = obs\n",
    "        action = model.sample(a0).detach()\n",
    "        #action = normalize(action)\n",
    "        # max - min\n",
    "        \n",
    "        obs, reward, done, truncated,reste = train_env.step(addnoise(action, noise_std))\n",
    "        \n",
    "        #model.updateB(obs[0],obs[1],obs[2],obs[3])\n",
    "        \n",
    "        # Compute losses\n",
    "        with torch.no_grad(): v1, _ = model(obs)\n",
    "        target_v = addnoise(reward, noise_std) + model.discount_factor*v1\n",
    "        optim_critic.zero_grad()\n",
    "        critic_loss, n_update = model.compute_critic_loss(v0, target=target_v)\n",
    "        critic_loss.backward()\n",
    "        optim_critic.step()\n",
    "        if target_v - v0 > 0: \n",
    "            for _ in range(n_update):\n",
    "                optim_actor.zero_grad()\n",
    "                mse_loss(normalize(model(obs0)[1]), action).backward()\n",
    "                optim_actor.step()\n",
    "        if done or truncated: obs,_ = train_env.reset()\n",
    "        if it % eval_interval == 0:\n",
    "            perf = test(eval_env, model, n_test, noise_std)\n",
    "            scores.append((it, perf))\n",
    "            print(f'{it = } | reward {perf}')\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "# Initialisation\n",
    "\n",
    "def make_env(env_name, max_episode_steps= 500):\n",
    "    return TimeLimit(gymnasium.make('CartpoleEnvCacla'), max_episode_steps)\n",
    "\n",
    "env_name = 'CartpoleEnvCacla'\n",
    "max_episode = 500\n",
    "train_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "eval_env = make_env(env_name, max_episode_steps=max_episode)\n",
    "\n",
    "all_perfs = []\n",
    "n_runs = 1\n",
    "discount_factor = 0.9\n",
    "noise_std= 0.3\n",
    "exploration = 'gaussian'\n",
    "var = False\n",
    "path = f'log/{exploration}/n_novar_gamma{int(discount_factor*100)}_std{noise_std}'\n",
    "\n",
    "if not os.path.exists(path): \n",
    "    print()\n",
    "    os.mkdir(path)\n",
    "\n",
    "for run in range(1,n_runs+1):\n",
    "    print(f'======= {run = } / {n_runs} =========')\n",
    "    cacla = Cacla(in_dim=4, h_dim=12, \n",
    "                  discount_factor=discount_factor, \n",
    "                  gaussian_noise=0.1,\n",
    "                 var=var,\n",
    "                 exploration=exploration)\n",
    "    optim_crit = SGD(cacla.critic.parameters(), lr=0.01)\n",
    "    optim_act = SGD(cacla.actor.parameters(), lr=0.01)\n",
    "    optim = (optim_crit, optim_act)\n",
    "    scores = train(eval_env, train_env, cacla, optim, \n",
    "                   eval_interval=1024, \n",
    "                   step_max=102400,\n",
    "                  n_test=10,\n",
    "                  noise_std= noise_std)\n",
    "    scores = np.array(scores)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.savetxt(path+f'/{timestr}.log', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = make_env('CartpoleEnvArticle')\n",
    "xmax = np.zeros(4)\n",
    "for _ in range(100):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    obs0,_ = env_test.reset()\n",
    "    xmax = np.maximum(xmax, np.abs(obs0))\n",
    "    while not done and not truncated:\n",
    "        obs0, reward, done,truncated,_ = env_test.step(1)\n",
    "        xmax = np.maximum(xmax, np.abs(obs0))\n",
    "xmax.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
